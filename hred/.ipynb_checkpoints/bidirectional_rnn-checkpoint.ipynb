{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The bi-directional RNN that will form the basis of all our future dialogue\n",
    "models.\n",
    "\n",
    "The goal is to create a bi-directional encoder-decoder that can be either used independently\n",
    "for next-response generation, or integrated into a hierarchical (or more complicated)\n",
    "model.\n",
    "\n",
    "To that end, the BidirectionalRNN class should should support the same interface as our\n",
    "other dialogue models: it should take a Config() object, it should take a DialogueCorpus()\n",
    "object, and it should support the same fit() and predict() methods that (along with SciPy\n",
    "classifiers) all our models support.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "1. we currently ignore encoder/decoder depth -- fix this\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "x  finish build-network function for encoder\n",
    "x  copy and write decoder build-network function on same pattern\n",
    "*3.[give both individual fit and predict functions\\\n",
    "4. write compose of both as BidirectionalEncoderDecoderRNN\n",
    "5. alter DialogueCorpus so it takes into account whether or no the model being trained is hierarchical\n",
    "   (i.e., wants whole dialogues) or flat (i.e., wants only adjacency pairs)\n",
    "\"\"\"\n",
    "\n",
    "# Keras packages\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Bidirectional\n",
    "\n",
    "# Our packages\n",
    "from config import Config\n",
    "from dialogue_corpus import DialogueCorpus\n",
    "\n",
    "class EmbedOHE(Dense):\n",
    "    \"\"\"\n",
    "    A Dense layer that we can use to embed one-hot word sequences, merely\n",
    "    dressed up semantically\n",
    "    \"\"\"\n",
    "\n",
    "class BidirectionalEncoderRNN:\n",
    "    def __init__(self, config=Config(), corpus=None):\n",
    "        self.config = config\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        if(self.corpus is None):\n",
    "            self.corpus = DialogueCorpus(config)\n",
    "        \n",
    "        self.build() # this will be a Keras model for now\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        The encoder computational graph consists of three components:\n",
    "        (1) the input node            `encoder_input`\n",
    "        (2) the Recurrent part        `encoder_rnn`\n",
    "        (3) the hidden state output   `encoder_hidden_state`\n",
    "        \"\"\"\n",
    "        \n",
    "        # Grab hyperparameters from self.config:\n",
    "        hidden_dim = self.config['encoding-layer-width']\n",
    "        recurrent_unit = self.config['recurrent-unit-type']\n",
    "        bidirectional = self.config['encoding-layer-bidirectional']\n",
    "        embedding_dim = self.config['embedding-dim']\n",
    "        vocab_size = self.config['vocab-size']\n",
    "        \n",
    "        # Assemble the network components:\n",
    "        encoder_input = Input(shape=(None, vocab_size))\n",
    "        encoder_embed = EmbedOHE(embedding_dim, activation=None, use_bias=False)(encoder_input)\n",
    "        encoder_rnn, encoder_hidden_state = None, None\n",
    "        \n",
    "        if recurrent_unit == 'lstm':\n",
    "            encoder_rnn = LSTM(hidden_dim, return_state=True)\n",
    "            encoder_outputs, encoder_state_h, encoder_state_c = encoder_rnn(encoder_embed)\n",
    "            # discard the encoder output, keeping only the hidden state\n",
    "            encoder_hidden_state = [encoder_state_h, encoder_state_c]\n",
    "        if recurrent_unit == 'gru':\n",
    "            encoder_rnn = GRU(hidden_dim, return_state=True)\n",
    "            encoder_outputs, encoder_hidden_state = encoder_rnn(encoder_embed)\n",
    "        else:\n",
    "            raise Exception('Invalid recurrent unit type: {}'.format(recurrent_unit))\n",
    "        \n",
    "        # make the RNN component bidirectional, if desired\n",
    "        if bidirectional:\n",
    "            encoder_rnn = Bidirectional(encoder_rnn)\n",
    "        \n",
    "        # save the three Enccoder components as class state\n",
    "        self.encoder_input = encoder_input\n",
    "        self.encoder_rnn = encoder_rnn\n",
    "        self.encoder_hidden_state = encoder_hidden_state\n",
    "        \n",
    "        # finally, build the training model\n",
    "        self.training_model = Model(self.encoder_input, self.encoder_hidden_state)\n",
    "    \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "class BidirectionalDecoderRNN:\n",
    "    def __init(self, config=Config(), corpus=None):\n",
    "        self.config = config\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        if(self.corpus is None):\n",
    "            self.corpus = DialogueCorpus(config)\n",
    "        \n",
    "        self.build() # this will be a Keras model for now\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        The decoder computational graph consists of three components:\n",
    "        (1) the input node            `decoder_input`\n",
    "        (2) the Recurrent part        `decoder_rnn`\n",
    "        (3) the decoder output        `decoder_output`\n",
    "        \"\"\"\n",
    "        \n",
    "        # Grab hyperparameters from self.config:\n",
    "        hidden_dim = self.config['encoding-layer-width']\n",
    "        recurrent_unit = self.config['recurrent-unit-type']\n",
    "        bidirectional = self.config['encoding-layer-bidirectional']\n",
    "        vocab_size = self.config['vocab-size']\n",
    "        \n",
    "        # Assemble the network components:\n",
    "        decoder_inputs = Input(shape=(None, vocab_size))\n",
    "        \n",
    "        if recurrent_unit == 'lstm':\n",
    "            decoder_rnn = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _, _ = decoder_rnn(decoder_inputs,\n",
    "                                                initial_state=encoder_hidden_state)\n",
    "        elif recurrent_unit == 'gru':\n",
    "            decoder_rnn = GRU(hidden_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _ = decoder_rnn(decoder_inputs, \n",
    "                                             initial_state=encoder_hidden_state)\n",
    "        else:\n",
    "            raise Exception('Invalid recurrent unit type: {}'.format(recurrent_unit))\n",
    "        \n",
    "        # make the RNN component bidirectional, if desired\n",
    "        if bidirectional:\n",
    "            encoder_rnn = Bidirectional(encoder_rnn)\n",
    "        \n",
    "        decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        # save the four Decoder components as class state\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_rnn = decoder_rnn\n",
    "        self.decoder_dense = decoder_dense\n",
    "        self.decoder_output = decoder_output\n",
    "        \n",
    "        # build the training and inference models\n",
    "        self.training_model = Model()\n",
    "    \n",
    "class BidirectionalEncoderDecoderRNN:\n",
    "    def __init(self, config=Config(), corpus=None):\n",
    "        self.config = config\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        if(self.corpus is None):\n",
    "            self.corpus = DialogueCorpus(config)\n",
    "        \n",
    "        # build the encoder and decoder\n",
    "        self.encoder = BidirectionalEncoderRNN(self.config, self.corpus)\n",
    "        self.decoder = BidirectionalDecoderRNN(self.config, self.corpus)\n",
    "        \n",
    "        # combine them into training and inference models\n",
    "        self.build_training_model()\n",
    "        self.build_inference_model()\n",
    "        \n",
    "    def build_training_model(self):\n",
    "        if self.config['hierarchical']:\n",
    "            # do something\n",
    "        else:\n",
    "            self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "    \n",
    "    def build_inference_model(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        # grab some hyperparameters from our config\n",
    "        optimizer = self.config['optimizer']\n",
    "        loss = self.config['loss']\n",
    "        batch_size = self.config['batch-size']\n",
    "        num_epochs = self.config['num-epochs']\n",
    "        validation_split = self.config['validation-split']\n",
    "        \n",
    "        # grab the training and validation data\n",
    "        encoder_x = self.corpus.get_encoder_x()\n",
    "        decoder_x = self.corpus.get_decoder_y()\n",
    "        decoder_y = self.corpus.get_decoder_y()\n",
    "        \n",
    "        self.training_model.compile(optimizer=optimizer, loss=loss)\n",
    "        self.training_model.fit([encoder_x, decoder_x], decoder_y,\n",
    "                                batch_size=batch_size\n",
    "                                epochs=num_epochs\n",
    "                                validation_split=validation_split)\n",
    "        \n",
    "    def response(self, input_sentence):\n",
    "        pass\n",
    "    \n",
    "    def converse(self):\n",
    "        # begins a loop that allows the user to converse with the machine\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# make a BRNN\n",
    "enc = BidirectionalEncoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, None, 15000)       0         \n",
      "_________________________________________________________________\n",
      "embed_ohe_8 (EmbedOHE)       (None, None, 512)         7680000   \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  [(None, 512), (None, 512) 1574400   \n",
      "=================================================================\n",
      "Total params: 9,254,400\n",
      "Trainable params: 9,254,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc.training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = BidirectionalDecoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec.bu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
