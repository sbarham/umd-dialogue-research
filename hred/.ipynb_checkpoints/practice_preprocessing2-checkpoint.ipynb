{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "from config import Config\n",
    "\n",
    "\"\"\"\n",
    "DONE: add the <pad> character to the vocab so that it gets included in the ie/ohe encoders\n",
    "DONE: go through the t_train_dia/t_train_utt/t_test_dia/t_test_utt and pad all seqs\n",
    "DONE: implement a vectorize/devectorize_dialogue() function\n",
    "DONE/*: implement the next_batch function, which should also return original sequence lengths:\n",
    "          so four outputs -- batch_enc_in, batch_dec_in, batch_dec_out, seqlens\n",
    "DONE: implement optional sequence trimming (rather discarding very long sequences) --\n",
    "        this is crucial, because the longest dialogue is 89 turns long, and the longest utterance\n",
    "        is basically a 684-word essay; that means that, given a modest 10000-word vocabulary,\n",
    "        a single dialogue, one-hot encoded, takes ~ 100 * 600 * 10,000 * 4 bytes to represent,\n",
    "        i.e., 2.4GB. Given a batch size of, say 64 (i.e., ~50, which is trendy these days), well,\n",
    "        that's impracticable to say the least. That's ~150GB for a single batch of ohe dialogues.\n",
    "        Thus, it's crucial to limit the size of the representation by discarding outliers\n",
    "        in terms of sequence length (whether dialogue or utterance length).\n",
    "        \n",
    "TODO: finish next_batch(), such that it returns also the decoder inputs/outputs\n",
    "\"\"\"\n",
    "\n",
    "class DialogueCorpus:\n",
    "    def __init__(self, config=Config()):\n",
    "        # load configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # initialize corpus-related parameters\n",
    "        self.corpus_loaded = False\n",
    "        self.pad_u = '<pad_u>'\n",
    "        self.pad_d = '<pad_d>'\n",
    "        \n",
    "        # initialize training bookkeeping parameters\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        \n",
    "        # load the corpus and make the training/test split\n",
    "        self.load_corpus()\n",
    "        \n",
    "        # pad and vectorize the dialogue samples, remembering their original length\n",
    "        self.process_corpus()\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def load_corpus(self):\n",
    "        if self.corpus_loaded:\n",
    "            return\n",
    "        \n",
    "        # load the corpus into memory\n",
    "        self.read_input_data(self.config['path-to-corpus'])\n",
    "        \n",
    "        # split the corpus into train and test samples\n",
    "        self.t_train_dia, self.t_test_dia = train_test_split(self.t_dialogues,\n",
    "                                                     train_size=self.config['train-test-split'],\n",
    "                                                     random_state=self.config['random-state'])\n",
    "        \n",
    "        # record num samples\n",
    "        self.num_train_samples = len(self.t_train_dia)\n",
    "        self.num_test_samples = len(self.t_test_dia)\n",
    "        \n",
    "        # get the flattened utterance lists for the train and test sets\n",
    "        self.t_train_utt = [utt for dia in self.t_train_dia for utt in dia]\n",
    "        self.t_test_utt = [utt for dia in self.t_test_dia for utt in dia]\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def reload_corpus(self):\n",
    "        self.corpus_loaded = False\n",
    "        self.load_corpus()\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def process_corpus(self):\n",
    "        # record the original sequence lengths for all dialogues and utterances\n",
    "        # in both the train and test sets\n",
    "        self.record_seq_lengths()\n",
    "        \n",
    "        # initialize the integer and one-hot encoders\n",
    "        self.init_ohe()\n",
    "        \n",
    "        # pad the sequences\n",
    "        self.pad_sequences()\n",
    "        \n",
    "        # now pad and vectorize the sequences\n",
    "        # self.vectorize_sequences()\n",
    "        # ^ we no longer do this en masse b/c it takes too much memory;\n",
    "        # rather, we vectorize individual batches on the fly when requested\n",
    "        # via next_batch\n",
    "        \n",
    "        # finally, mark that the corpus was successfully loaded\n",
    "        self.corpus_loaded = True\n",
    "        \n",
    "        return\n",
    "                \n",
    "    ######################################\n",
    "    #      READING IN THE DIALOGUE\n",
    "    ######################################\n",
    "    \n",
    "    def read_input_data(self, path_to_corpus):\n",
    "        with open(path_to_corpus, 'r') as f:\n",
    "            lines = list(f)\n",
    "\n",
    "        # each line is a discrete dialogue, with utterances\n",
    "        # tab-delimited\n",
    "        # dialogues = []\n",
    "        t_dialogues = []\n",
    "        # utterances = []\n",
    "        # t_utterances = []\n",
    "        words = [self.pad_u, self.pad_d]\n",
    "        \n",
    "        # for filtering out long dialogues and utterances, we'll need these settings:\n",
    "        max_dl = self.config['max-dialogue-length']\n",
    "        use_max_dl = not (self.config['use-corpus-max-dialogue-length'])\n",
    "        max_ul = self.config['max-utterance-length']\n",
    "        use_max_ul = not (self.config['use-corpus-max-utterance-length'])\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            dialogue = line.split('\\t')[:-1]\n",
    "            \n",
    "            # filter out really really long dialogues\n",
    "            if use_max_dl and len(dialogue) > max_dl:\n",
    "                continue\n",
    "\n",
    "            t_dialogue = []\n",
    "            keep_dia = True\n",
    "            for utt in dialogue:\n",
    "                t_utt = utt.split(' ')\n",
    "                \n",
    "                # filter out dialogues with really really long utterances\n",
    "                if use_max_ul and len(t_utt) > max_ul:\n",
    "                    keep_dia = False\n",
    "                    break\n",
    "                \n",
    "                t_dialogue += [t_utt]\n",
    "                words += t_utt\n",
    "\n",
    "            # If the dialogue had utterances that were too large, skip it ...\n",
    "            if not keep_dia:\n",
    "                continue\n",
    "            \n",
    "            # ... otherwise, keep it:\n",
    "            # dialogues += [dialogue] # add the dialogue (i.e., list of utterances)\n",
    "            # utterances += dialogue # add the utterances to the flattened list of all utterances\n",
    "            t_dialogues += [t_dialogue]\n",
    "            # t_utterances += t_dialogue\n",
    "        \n",
    "        # Record the accumulated dialogues, utterances, tokenized dialogues,\n",
    "        # and tokenized utterances\n",
    "        self.t_dialogues = np.array(t_dialogues)\n",
    "        # self.t_utterances = np.array(t_utterances)\n",
    "        \n",
    "        # Record the list of tokens and the list of vocab items\n",
    "        self.words = words\n",
    "        self.vocab = list(set(words))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    ######################################\n",
    "    # RECORDING SEQLEN and (DE)VECTORIZING\n",
    "    ######################################\n",
    "    \n",
    "    def record_seq_lengths(self):\n",
    "        \"\"\"\n",
    "        The lengths of dialogue- and utterance-level sequences are\n",
    "        recorded as:\n",
    "            self.train_dia_seqlens (lengths of training dialogues)\n",
    "            self.train_utt_seqlens (lengths of training utterances)\n",
    "            self.test_dia_seqlens  (lengths of test dialogues)\n",
    "            self.test_utt_seqlens  (lengths of test dialogues)\n",
    "        \"\"\"\n",
    "        sample_sets = ['train', 'test']\n",
    "        \n",
    "        for sample_set in sample_sets:\n",
    "            t_dialogues = getattr(self, 't_' + sample_set + '_dia')\n",
    "            # record the original seqlens at the utterance and dialogue level\n",
    "            seqlens_dia = []\n",
    "            seqlens_utt = []\n",
    "            seqlens_utt_flat = []\n",
    "            for t_dialogue in t_dialogues:\n",
    "                # get dialogue length\n",
    "                seqlens_dia += [len(t_dialogue)]\n",
    "\n",
    "                # get constituent utterances lengths\n",
    "                lens = [len(u) for u in t_dialogue]\n",
    "                seqlens_utt += [lens]\n",
    "                seqlens_utt_flat += lens\n",
    "\n",
    "            setattr(self, sample_set + '_dia_seqlens', seqlens_dia)\n",
    "            setattr(self, sample_set + '_utt_seqlens', seqlens_utt)\n",
    "            # setattr(self, sample_set + '_utt_flat_seqlens', seqlens_utt_flat)\n",
    "            setattr(self, sample_set + '_dia_maxlen', max(seqlens_dia))\n",
    "            setattr(self, sample_set + '_utt_maxlen', max(seqlens_utt_flat))\n",
    "            \n",
    "        self.max_dialogue_length = max(self.train_dia_maxlen, self.test_dia_maxlen)\n",
    "        self.max_utterance_length = max(self.train_utt_maxlen, self.test_utt_maxlen)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def init_ohe(self):\n",
    "        \"\"\"\n",
    "        From here on out,\n",
    "            - 'ie' stands for 'integer encoded', and\n",
    "            - 'ohe' stands for 'one-hot encoded'\n",
    "        \"\"\"\n",
    "        # create the encoders\n",
    "        self.ie = LabelEncoder()\n",
    "        self.ohe = OneHotEncoder(sparse=False)\n",
    "        \n",
    "        # fit the encoders to the corpus vocabulary\n",
    "        self.ie_vocab = self.ie.fit_transform(self.vocab)\n",
    "        self.ohe_vocab = self.ohe.fit_transform(self.ie_vocab.reshape(len(self.ie_vocab), 1))\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def pad_sequences(self):\n",
    "        pad_d = '<pad_d>' # used to fill empty dialogue turn\n",
    "        pad_u = '<pad_u>' # used to pad an utterance\n",
    "        \n",
    "        empty_turn = [pad_d] * self.max_utterance_length\n",
    "        \n",
    "        sample_sets = ['train', 'test']\n",
    "        for sample_set in sample_sets:\n",
    "            dia_seqlens = getattr(self, sample_set + '_dia_seqlens')\n",
    "            utt_seqlens = getattr(self, sample_set + '_utt_seqlens')\n",
    "            \n",
    "            for i, lens in enumerate(utt_seqlens):\n",
    "                for j, length in enumerate(lens):\n",
    "                    utt_diff = self.max_utterance_length - length\n",
    "                    getattr(self, 't_' + sample_set + '_dia')[i][j] += [pad_u] * utt_diff\n",
    "                dia_diff = self.max_dialogue_length - dia_seqlens[i]\n",
    "                getattr(self, 't_' + sample_set + '_dia')[i] += [empty_turn] * dia_diff\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def pretty_print_dialogue(self, dia):\n",
    "        for utt in dia:\n",
    "            if utt[0] == self.pad_d:\n",
    "                break\n",
    "            print(self.prettify_utterance(utt))\n",
    "                \n",
    "        return\n",
    "                      \n",
    "    def prettify_utterance(self, utt):\n",
    "        return ' '.join([w for w in utt if not w == self.pad_u])\n",
    "        \n",
    "    \"\"\"\n",
    "    This should have been obvious to the thinking man, but any reasonable dialogue corpus will be\n",
    "    *far* too big to one-hot encode all in one go -- think 10,000 word vocabulary x 4,000,000 words x\n",
    "    4 bytes per ohe-vector entry: that's 10 * 4 * 4 160 GB of one-hot vectors. That *will* fit in\n",
    "    our Azure supercomputer's memory (it has a memory of 240GB), but it makes testing impossible\n",
    "    on any other machine (and the Azure machine is far too expensive to use for testing). Instead,\n",
    "    we'll have to vectorize o demand, on the fly.\n",
    "    \"\"\"\n",
    "#     def vectorize_sequences(self):\n",
    "#         sample_sets = ['train', 'test']\n",
    "#         for sample_set in sample_sets:\n",
    "#             t_dialogues_vec = []\n",
    "#             t_dialogues = getattr(self, 't_' + sample_set + '_dia')\n",
    "            \n",
    "#             for i, dia in enumerate(t_dialogues):\n",
    "#                 t_dialogues_vec += [[self.vectorize_utterance(utt) for utt in dia]]\n",
    "            \n",
    "#             setattr(self, 't_' + sample_set + '_dia_vec', t_dialogues_vec)\n",
    "\n",
    "    def vectorize_batch(self, batch):\n",
    "        return np.array([self.vectorize_dialogue(dia) for dia in batch])\n",
    "    \n",
    "    def vectorize_dialogue(self, dia):\n",
    "        # we squeeze it because it's coming out with an extra empty\n",
    "        # dimension at the front of the shape: (1 x dia x utt x word)\n",
    "        return np.array([[self.vectorize_utterance(utt) for utt in dia]]).squeeze()\n",
    "        \n",
    "    def vectorize_utterance(self, utterance):\n",
    "        \"\"\"\n",
    "        Take in a tokenized utterance and transform it into a sequence of one-hot vectors\n",
    "        \"\"\"\n",
    "        ie_utterance = self.ie.transform(utterance)\n",
    "        ohe_utterance = self.ohe.transform(ie_utterance.reshape(len(ie_utterance), 1))\n",
    "        \n",
    "        return ohe_utterance\n",
    "\n",
    "    def devectorize_utterance(self, ohe_utterance):\n",
    "        \"\"\"\n",
    "        Take in a sequence of one-hot vectors and transform it into a tokenized utterance\n",
    "        \"\"\"\n",
    "        ie_utterance = [argmax(w) for w in ohe_utterance]\n",
    "        utterance = self.le.inverse_transform(ie_utterance)\n",
    "        \n",
    "        return utterance\n",
    "    \n",
    "    ######################################\n",
    "    # GETTING THE NEXT BATCH IN TRAINING\n",
    "    ######################################\n",
    "    \n",
    "    def next_batch(self):\n",
    "        start = self._index_in_epoch\n",
    "        \n",
    "        # Shuffle for the first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and self.config['shuffle']:\n",
    "            perm = np.arange(self.num_train_samples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._train = self.t_train_dia[perm]\n",
    "            self._seqlens = [self.train_utt_seqlens[i] for i in perm]\n",
    "        \n",
    "        # If we're out of training samples ...\n",
    "        if start + self.config['batch-size'] > self.num_train_samples:\n",
    "            # ... then we've finished the epoch\n",
    "            self._epochs_completed += 1\n",
    "            \n",
    "            # Gather the leftover dialogues from this epoch\n",
    "            num_leftover_samples = self.num_train_samples - start\n",
    "            leftover_dialogues = self._train[start:self.num_train_samples]\n",
    "            leftover_seqlens = self._seqlens[start:self.num_train_samples]\n",
    "            \n",
    "            # Get a new permutation of the training dialogues\n",
    "            if self.config['shuffle']:\n",
    "                perm = numpy.arange(self.num_train_samples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._train = self.t_train_dia[perm]\n",
    "                self._seqlens = [self.train_utt_seqlens[i] for i in perm]\n",
    "                \n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            \n",
    "            # Put together our batch from leftover and new dialogues\n",
    "            new_dialogues = self._train[start:end]\n",
    "            new_seqlens = self._seqlens[start:end]\n",
    "            batch = np.concatenate((leftover_dialogues, new_dialogues), axis=0)\n",
    "            seqlens = np.concatenate((leftover_seqlens, new_seqlens), axis=0)\n",
    "            \n",
    "            # prepare the decoder input/output\n",
    "            #TODO\n",
    "            \n",
    "            # release the processed batch\n",
    "            return (self.vectorize_batch(batch), seqlens)\n",
    "        else:\n",
    "            # update the current index in the training data\n",
    "            end = self._index_in_epoch + self.config['batch-size']\n",
    "            self._index_in_epoch = end\n",
    "            \n",
    "            # get the next batch\n",
    "            batch = self._train[start:end]\n",
    "            seqlens = self._seqlens[start:end]\n",
    "            \n",
    "            # prepare the decoder input/output\n",
    "            #TODO\n",
    "            \n",
    "            # release the processed batch\n",
    "            return (self.vectorize_batch(batch), seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = DialogueCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sir , i ran the <unk> in high school .\n",
      "he 's fast , captain , i saw him .\n"
     ]
    }
   ],
   "source": [
    "data.pretty_print_dialogue(data.t_train_dia[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dialogue length:\n",
    "len(data.t_train_dia[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utterance length:\n",
    "len(data.t_train_dia[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max_dialogue_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max_utterance_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.t_test_dia[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.t_test_dia[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all seems to be working perfectly so far\n",
    "# the only thing that remains is to implement vectorization -- and\n",
    "# then, of course, the all-important next_batch method <7 March, 2018>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dia1 = data.t_train_dia[72]\n",
    "dia2 = data.t_train_dia[1234]\n",
    "dia3 = data.t_train_dia[14555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we 're getting everyone together as fast as we can .\n",
      "what does the letter say ?\n",
      "they want us to take our missiles out of turkey along with the no invasion <unk> . it looks like <unk> was a <unk> after all , and they were just stalling for time .\n"
     ]
    }
   ],
   "source": [
    "data.pretty_print_dialogue(dia1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you got a lot of guts calling me .\n",
      "yes i do . that 's all i got . guts and you , johnson . there 's no one else i can trust .\n",
      "you made a fool out of me ! in front of the fbi !\n",
      "listen , this ai n't about us ! it 's about a thirteen year old girl . she 's not gon na make it . unless we do something .\n"
     ]
    }
   ],
   "source": [
    "data.pretty_print_dialogue(dia2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so am i .\n",
      "listen , i think i 'm hit bad .\n",
      "where 'd they get you ?\n",
      "damn you , lieutenant <unk> ! lothar !\n"
     ]
    }
   ],
   "source": [
    "data.pretty_print_dialogue(dia3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dia_vec1 = data.vectorize_dialogue(dia1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40, 10003)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia_vec1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = [dia1] + [dia2] + [dia3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_tensor = data.vectorize_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 40, 10003)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit batch, seqlens = data.next_batch()\n",
    "# 6.45 s ± 32.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seqlens = data.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10, 40, 10003)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 7]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Everything's working now! <the wee morning hours of 8 March, 2018>\n",
    "# The only thing left at this point is to process the decoder inputs/outputs --\n",
    "# which just involves shifting things by one time step and adding start/stop symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 7],\n",
       " [2, 3, 8],\n",
       " [9, 7],\n",
       " [7, 12, 9],\n",
       " [10, 14],\n",
       " [8, 4, 12],\n",
       " [6, 8, 13, 21, 10],\n",
       " [6, 21],\n",
       " [2, 40, 9],\n",
       " [11, 13],\n",
       " [5, 4, 6],\n",
       " [19, 4, 14, 5, 10, 7, 7, 22],\n",
       " [9, 5, 26, 5],\n",
       " [13, 33, 11],\n",
       " [3, 7],\n",
       " [29, 9],\n",
       " [25, 30],\n",
       " [10, 6, 16],\n",
       " [4, 33],\n",
       " [2, 4],\n",
       " [7, 5],\n",
       " [31, 7, 31, 23],\n",
       " [5, 2, 19, 4, 8, 11, 33, 19, 5, 20],\n",
       " [2, 6, 5, 5, 2, 4, 2, 2, 5],\n",
       " [3, 9],\n",
       " [7, 2, 20, 32],\n",
       " [8, 9, 3, 4, 33, 7, 14, 4],\n",
       " [7, 8],\n",
       " [36, 4, 10],\n",
       " [9, 14, 8],\n",
       " [10, 3, 2, 3],\n",
       " [28, 5, 10],\n",
       " [26, 3],\n",
       " [15, 8],\n",
       " [15, 4, 12, 8],\n",
       " [9, 33, 11],\n",
       " [18, 19],\n",
       " [7, 3, 22],\n",
       " [16, 6],\n",
       " [26, 4, 13, 2, 2, 2, 8, 4, 15],\n",
       " [9, 9, 6],\n",
       " [10, 9, 8, 4, 9, 9],\n",
       " [5, 10],\n",
       " [35, 6, 18, 14],\n",
       " [24, 16],\n",
       " [16, 16, 6],\n",
       " [8, 5, 8],\n",
       " [12, 4, 5, 2, 7, 14, 28, 9],\n",
       " [8, 5],\n",
       " [15, 7],\n",
       " [9, 9],\n",
       " [12, 15, 5],\n",
       " [8, 8, 4],\n",
       " [17, 5],\n",
       " [10, 17],\n",
       " [5, 7],\n",
       " [21, 5, 22, 5, 30],\n",
       " [3, 3],\n",
       " [11, 11, 15],\n",
       " [14, 8, 2],\n",
       " [39, 10],\n",
       " [5, 3],\n",
       " [18, 15],\n",
       " [8, 6]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
