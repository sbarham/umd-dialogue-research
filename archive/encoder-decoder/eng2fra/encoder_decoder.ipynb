{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense\n",
    "import numpy as np\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, data, model_path, latent_dim=512, batch_size=64, \n",
    "                 epochs=10, validation_split=0.2, rnn_type='lstm', optimizer='rmsprop'):\n",
    "        self.data = data\n",
    "        self.model_path = model_path\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.validation_split = validation_split\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        # now build the training and inference networks\n",
    "        build_training_model()\n",
    "        build_inference_model()\n",
    "        \n",
    "    def build_training_model(self):\n",
    "        # the encoder\n",
    "        encoder_inputs = Input(shape=(None, self.data.input_max_len))\n",
    "        encoder_rnn, encoder_hidden_state = None, None\n",
    "        \n",
    "        if self.rnn_type == 'lstm':\n",
    "            self.encoder_rnn = LSTM(self.latent_dim, return_state=True)\n",
    "            encoder_outputs, encoder_state_h, encoder_state_c = encoder_rnn(encoder_inputs)\n",
    "            # discard the encoder output, keeping only the hidden state\n",
    "            encoder_hidden_state = [encoder_state_h, encoder_state_c]\n",
    "        else:\n",
    "            encoder_rnn = GRU(self.latent_dim, return_state=True)\n",
    "            encoder_outputs, encoder_hidden_state = encoder_rnn(encoder_inputs)\n",
    "        \n",
    "        # the decoder\n",
    "        decoder_inputs = Input(shape=(None, self.data.output_max_len))\n",
    "        \n",
    "        if self.rnn_type == 'lstm':\n",
    "            decoder_rnn = LSTM(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _, _ = decoder_rnn(decoder_inputs,\n",
    "                                                initial_state=encoder_hidden_state)\n",
    "        else:\n",
    "            decoder_rnn = GRU(self.latent_dim, return_sequences=True, return_state=True)\n",
    "            decoder_outputs, _ = decoder_rnn(decoder_inputs, \n",
    "                                             initial_state=encoder_hidden_state)\n",
    "        \n",
    "        decoder_dense = Dense(self.data.num_output_chars, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        # save the network as attributes and build the training model\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_rnn = encoder_rnn\n",
    "        self.encoder_hidden_state = encoder_hidden_state\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_rnn = decoder_rnn\n",
    "        self.decoder_dense = decoder_dense\n",
    "        self.decoder_outputs = decoder_outputs\n",
    "        \n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "\n",
    "\n",
    "    def build_inference_model(self):\n",
    "        # the inference model actually consists of two discrete sub-models --\n",
    "        # the encoder ...\n",
    "        self.encoder_model = Model(self.encoder_inputs, self.encoder_hidden_state)\n",
    "        \n",
    "        # ... and the decoder\n",
    "        decoder_hidden_state_input = None\n",
    "        decoder_outputs = None\n",
    "        decoder_state = None\n",
    "        if self.rnn_type == 'lstm':\n",
    "            decoder_hidden_state_input_h = Input(shape=(self.latent_dim,))\n",
    "            decoder_hidden_state_input_c = Input(shape=(self.latent_dim,))\n",
    "            decoder_hidden_state_input = [decoder_hidden_state_input_h, decoder_hidden_state_input_c]\n",
    "            # take in the regular inputs, condition on the hidden state\n",
    "            decoder_outputs, state_h, state_c = self.decoder_rnn(self.decoder_inputs,\n",
    "                                                                initial_state=decoder_hidden_state_input)\n",
    "            decoder_state = [state_h, state_c]\n",
    "        else:\n",
    "            decoder_hidden_state_input = Input(shape=(self.latent_dim,))\n",
    "            # take in the regular inputs, condition on the hidden state\n",
    "            decoder_outputs, hidden_state = self.decoder_rnn(self.decoder_inputs,\n",
    "                                                            initial_state=decoder_hidden_state_input)\n",
    "            decoder_state = hidden_state\n",
    "            \n",
    "        # run it through a dense softmax layer\n",
    "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_hidden_state_input,\n",
    "                                   [decoder_outputs] + decoder_state)\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        self.training_model.compile(optimizer=self.optimizer, loss='categorical_crossentropy')\n",
    "        self.training_model.fit([self.data.encoder_x, self.data.decoder_x], self.data.decoder_y,\n",
    "                                batch_size=self.batch_size,\n",
    "                                epochs=self.epochs,\n",
    "                                validation_split=self.validation_split)\n",
    "\n",
    "        \n",
    "    def predict(self, input_seq):\n",
    "        return decode_sequence(input_seq)\n",
    "\n",
    "\n",
    "    def translate(self, input_str):\n",
    "        # transform input_str into a numpy array\n",
    "        input_seq = np.zeros((1, self.data.input_max_len, self.data.num_input_chars))\n",
    "        for i, char in enumerate(input_str):\n",
    "            input_seq[1, i, char2index(char)] = 1\n",
    "        \n",
    "        # predict the translation using the inference model\n",
    "        return predict(input_seq)\n",
    "    \n",
    "    \n",
    "    def decode_sequence(self, input_seq):\n",
    "        # encode the input seq into a context vector\n",
    "        context_state = self.encoder_model.predict(input_seq)\n",
    "        \n",
    "        # create an empty target sequence, seeded with the start character\n",
    "        target_seq = np.zeros((1, 1, self.data.num_output_chars))\n",
    "        target_seq[0, 0, char2index('\\t')] = 1.\n",
    "        \n",
    "        output_str = ''\n",
    "        while True:\n",
    "            # decode the current sequence + current context into a\n",
    "            # conditional distribution over next token:\n",
    "            output_token_probs = None\n",
    "            if self.rnn_type == 'lstm':\n",
    "                output_token_probs, h, c = self.decoder_model.predict([target_seq] + context_state)\n",
    "                context_state = [h, c]\n",
    "            else:\n",
    "                output_token_probs, context_state = \\\n",
    "                    self.decoder_model.predict([target_seq] + context_state)\n",
    "            \n",
    "            # sample a token from the output distribution\n",
    "            sampled_token_index = np.argmax(output_token_probs[0, -1, :])\n",
    "            sampled_char = index2char(sampled_token_index)\n",
    "            \n",
    "            # add the sampled token to our output string\n",
    "            output_str += sampled_char\n",
    "            \n",
    "            # exit condition: either we've\n",
    "            # - hit the max length (self.data.output_max_len), or\n",
    "            # - decoded a stop token ('\\n')\n",
    "            if (sampled_char == '\\n' or\n",
    "                len(output_str) >= self.data.output_max_len):\n",
    "                break\n",
    "                \n",
    "            # update the np array (target seq)\n",
    "            target_seq = np.zeros((1, 1, self.data.num_output_chars))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "            \n",
    "        return output_str\n",
    "            \n",
    "\n",
    "    def save(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        \n",
    "        self.training_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
